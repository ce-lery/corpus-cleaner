#include <gtest/gtest.h>
#include "../corpus_cleaner/corpus_cleaner.hpp"
#include "../corpus_cleaner/util.hpp"
#include "../corpus_cleaner/normalizer.hpp"

// namespace {

class CorpusCleanerTest : public ::testing::Test {
protected:
    // You can freely delete empty functions in the following functions.
    uint32_t min_length=5;
    uint32_t max_length = 1000;
    set<string> accept_language{"__label__ja"};
    Document document;
    string sentence="";
    const string output_path = "../data/output/";
    const string intermediate_path = "../data/intermediate/"; 
    CorpusCleanerTest() {
        // Write the setup that will be executed for each test here.
    }

    virtual ~CorpusCleanerTest() {
        // Write here the clean-up that will be executed 
        //after each test and will not throw an exception.
    }

    // When constructor and destructor are not enough.
    // You can define the following methods:

    virtual void SetUp() {
        // This code is right after the constructor (just before each test)
        // will be called.
        mkdir(output_path.c_str(), 0777);
        mkdir(intermediate_path.c_str(), 0777);
    }

    virtual void TearDown() {
        // This code is placed immediately after each test (just before the destructor)
        // will be called.
        RemoveFolder(output_path);
        RemoveFolder(intermediate_path);
    }

    // Objects declared here are available to all tests within the test case.
    //string a;
    //uin32_t b;
};

// } //namespace

bool CompareFiles(const string& file1, const string& file2) {
    ifstream f1(file1, ios::binary);
    ifstream f2(file2, ios::binary);

    //compare file size
    if(filesystem::file_size(file1)!=filesystem::file_size(file2))  return false;

    // compare file contents
    if (f1 && f2) {
        char c1, c2;
        while (f1.get(c1) && f2.get(c2)) {
            if (c1 != c2)   return false; //file1!=file2
        }
        return true; //file1==file2
    } 
    else    return false; //can't open file, or can't find file.
}

TEST_F(CorpusCleanerTest, LengthFilter) {
    GenerateDedupLSH generate_dedup_lsh(5,200,20,10);
    LSHDeduplicator deduplicator(true,"../data/output/blacklist.txt",true,5120);
    CorpusCleaner corpus_cleaner("../data/input/",
                                 "../data/output/",
                                 min_length,
                                 max_length,
                                 accept_language,
                                 true,
                                 true,
                                 0.3,
                                 15000,
                                 &generate_dedup_lsh,
                                 &deduplicator);

    document.is_rejected=false;
    for(int i=0;i<1001;i++) sentence+="ã‚";
    document.text = sentence;
    corpus_cleaner.LengthFilter(document);
    ASSERT_TRUE(document.text == sentence);
    ASSERT_TRUE(document.is_rejected==true);

    document.is_rejected=false;
    document.text = "ã‚ã€‚";
    corpus_cleaner.LengthFilter(document);
    ASSERT_TRUE(document.text == "ã‚ã€‚");
    ASSERT_TRUE(document.is_rejected==true);

    document.text = "AAAã€‚";
    document.is_rejected=false;
    corpus_cleaner.LengthFilter(document);
    ASSERT_TRUE(document.text == "AAAã€‚");
    ASSERT_TRUE(document.is_rejected==true);

    document.text = "ã“ã‚“ã«ã¡ã¯ã€‚ã“ã‚“ã«ã¡ã¯ã€‚ã“ã‚“ã«ã¡ã¯ã€‚";
    document.is_rejected=false;
    corpus_cleaner.LengthFilter(document);
    ASSERT_TRUE(document.text == "ã“ã‚“ã«ã¡ã¯ã€‚ã“ã‚“ã«ã¡ã¯ã€‚ã“ã‚“ã«ã¡ã¯ã€‚");
    ASSERT_TRUE(document.is_rejected==false);

    document.text = "ã“ã‚“ã°ã‚“ã‚ã€‚";
    document.is_rejected=false;
    corpus_cleaner.LengthFilter(document);
    ASSERT_TRUE(document.text == "ã“ã‚“ã°ã‚“ã‚ã€‚");
    ASSERT_TRUE(document.is_rejected==false);
}


TEST_F(CorpusCleanerTest, ZeroPunctuationFilter) {
    GenerateDedupLSH generate_dedup_lsh(5,200,20,10);
    LSHDeduplicator deduplicator(true,"../data/output/blacklist.txt",true,5120);
    CorpusCleaner corpus_cleaner("../data/input/",
                                 "../data/output/",
                                 min_length,
                                 max_length,
                                 accept_language,
                                 true,
                                 true,
                                 0.3,
                                 15000,
                                 &generate_dedup_lsh,
                                 &deduplicator);

    document.is_rejected=false;
    document.text = "æ±äº¬ã€€å¤§é˜ªã€€åå¤å±‹ã€€æ¨ªæµœ";
    corpus_cleaner.ZeroPunctuationFilter(document);
    ASSERT_TRUE(document.is_rejected==true);

    document.is_rejected=false;
    document.text = "ã“ã‚“ã«ã¡ã‚ã€‚";
    corpus_cleaner.ZeroPunctuationFilter(document);
    ASSERT_TRUE(document.is_rejected==false);
    
    document.is_rejected=false;
    document.text = "1972å¹´5æœˆï¼šå¤§éŸ“èˆªç©ºãŒã‚½ã‚¦ãƒ«ç·šã§å°±èˆª ";
    corpus_cleaner.ZeroPunctuationFilter(document);
    ASSERT_TRUE(document.is_rejected==true);    
    
    document.is_rejected=false;
    document.text = "ã€Œå‹ã¦ã°ã‚ˆã‹ã‚ã†ãªã®ã ã‚¡ã‚¡ã‚¡ã‚¡ãƒƒ!!ã€";
    corpus_cleaner.ZeroPunctuationFilter(document);
    ASSERT_TRUE(document.is_rejected==false);

    document.is_rejected=false;
    document.text = "ã€ŒãŠã¾ãˆã¯â€¦â€¦è‡ªåˆ†ãŒã€æ‚ªã€ã ã¨æ°—ã¥ã„ã¦ã„ãªã„â€¦ã‚‚ã£ã¨ã‚‚ãƒ‰ã‚¹é»’ã„ã€æ‚ªã€ã ã€";
    corpus_cleaner.ZeroPunctuationFilter(document);
    ASSERT_TRUE(document.is_rejected==true);    

    document.is_rejected=false;
    document.text = "https://github.com/";
    corpus_cleaner.ZeroPunctuationFilter(document);
    ASSERT_TRUE(document.is_rejected==false);
}

TEST_F(CorpusCleanerTest, URLRemover) {
    GenerateDedupLSH generate_dedup_lsh(5,200,20,10);
    LSHDeduplicator deduplicator(true,"../data/output/blacklist.txt",true,5120);
    CorpusCleaner corpus_cleaner("../data/input/",
                                 "../data/output/",
                                 min_length,
                                 max_length,
                                 accept_language,
                                 true,
                                 true,
                                 0.3,
                                 15000,
                                 &generate_dedup_lsh,
                                 &deduplicator);
    document.is_rejected=false;
    document.text = "https://qiita.com/ã“ã‚Œã¯qiitaã®URLã§ã™";
    corpus_cleaner.URLRemover(document);
    ASSERT_TRUE(document.text == "ã“ã‚Œã¯qiitaã®URLã§ã™");
    ASSERT_TRUE(document.is_rejected==false);
    ASSERT_TRUE(document.metadata.find("URLRemover")!=document.metadata.end());

    document.text = "ã“ã‚Œã¯zennã®URLã§ã™https://zenn.dev/";
    corpus_cleaner.URLRemover(document);
    ASSERT_TRUE(document.text == "ã“ã‚Œã¯zennã®URLã§ã™");

    document.text = "https://zenn.dev/https://qiita.com/ã“ã‚Œã¯qiitaã¨zennã®URLã§ã™";
    corpus_cleaner.URLRemover(document);
    ASSERT_TRUE(document.text == "ã“ã‚Œã¯qiitaã¨zennã®URLã§ã™");

    document.text = "https://zenn.dev/ã‚https://qiita.com/ã„https://huggingface.co/ã†";
    corpus_cleaner.URLRemover(document);
    ASSERT_TRUE(document.text == "ã‚ã„ã†");

    document.text = "URLã«æ—¥æœ¬èªãŒå«ã¾ã‚Œã‚‹å ´åˆhttps://www.google.com/search?q=URL+%E6%97%A5%E6%9C%AC%E8%AA%9E&oq=URL+%E6%97%A5%E6%9C%AC%E8%AA%9E&aqs=chrome..69i57.3480j0j7&sourceid=chrome&ie=UTF-8";
    corpus_cleaner.URLRemover(document);
    ASSERT_TRUE(document.text == "URLã«æ—¥æœ¬èªãŒå«ã¾ã‚Œã‚‹å ´åˆ");
}

TEST_F(CorpusCleanerTest, MakeStats) {
    string input_path = "../data/input/test_URLRemover.txt";
    double elapsed_time = 11.56;
    string process_name = "URLRemover";
    Stats stats = MakeStats(process_name,input_path,elapsed_time);
    ASSERT_EQ("URLRemover",stats.process_name);
    ASSERT_EQ("",stats.file_name);
    ASSERT_EQ(elapsed_time,stats.elapsed_time);
    ASSERT_EQ(0,stats.result_file_size);
}

TEST_F(CorpusCleanerTest, SpecialCharacterRemover) {
    uint32_t min_length = 10;
    uint32_t max_length = 1000;
    set<string> accept_language{"__label__ja"};
    GenerateDedupLSH generate_dedup_lsh(5,200,20,10);
    LSHDeduplicator deduplicator(true,"../data/output/blacklist.txt",true,5120);
    CorpusCleaner corpus_cleaner("../data/input/",
                                 "../data/output/",
                                 min_length,
                                 max_length,
                                 accept_language,
                                 true,
                                 true,
                                 0.3,
                                 15000,
                                 &generate_dedup_lsh,
                                 &deduplicator);
                                 
    Document document;  
    document.text = "â˜€ã‚â†ã„âŒšã†â¤²ãˆâ­ãŠğŸ€€";
    cout << document.text << endl;
    document.language = "";
    document.language_score=0;
    corpus_cleaner.SpecialCharacterRemover(document);
    ASSERT_TRUE(document.text == "ã‚ã„ã†ãˆãŠ");
    ASSERT_TRUE(document.is_rejected == false);
    ASSERT_TRUE(document.metadata.find("SpecialCharacterRemover") != document.metadata.end());
    ASSERT_TRUE(document.language == "");       
    //ASSERT_TRUE(document.language_score == 0);
    
    document.text = "ãã‚ãã‚ç‹©ã‚‹ã‹â€¦â™ ";
    corpus_cleaner.SpecialCharacterRemover(document);
    ASSERT_TRUE(document.text == "ãã‚ãã‚ç‹©ã‚‹ã‹â€¦");
    
    document.text = "RTX4090æœ€é«˜â˜†â˜€â˜â™ ";
    corpus_cleaner.SpecialCharacterRemover(document);
    ASSERT_TRUE(document.text == "RTX4090æœ€é«˜");
     
    document.text = "ãŠè…¹ãŒç©ºãã¾ã—ãŸâ˜¹";
    corpus_cleaner.SpecialCharacterRemover(document);
    ASSERT_TRUE(document.text == "ãŠè…¹ãŒç©ºãã¾ã—ãŸ");
     
    document.text = "å¢ƒç•Œå€¤ã‚·ãƒªãƒ¼ã‚ºâ˜€âŸ¿â†â‡¿âŒ€â¿â¤€â¥¿â¬€â¯¿ğŸ€€ğŸƒ¿";
    corpus_cleaner.SpecialCharacterRemover(document);
    ASSERT_TRUE(document.text == "å¢ƒç•Œå€¤ã‚·ãƒªãƒ¼ã‚º");
}

TEST_F(CorpusCleanerTest, EmojiRemover) {
    GenerateDedupLSH generate_dedup_lsh(5,200,20,10);
    LSHDeduplicator deduplicator(true,"../data/output/blacklist.txt",true,5120);
    CorpusCleaner corpus_cleaner("../data/input/",
                                 "../data/output/",
                                 min_length,
                                 max_length,
                                 accept_language,
                                 true,
                                 true,
                                 0.3,
                                 15000,
                                 &generate_dedup_lsh,
                                 &deduplicator);
    document.text = "ã‚ˆã‚ã—ããŠé¡˜ã„ã—ã¾ã™ğŸ˜";
    document.language = "";
    document.language_score=0;
    corpus_cleaner.EmojiRemover(document);
    ASSERT_TRUE(document.text == "ã‚ˆã‚ã—ããŠé¡˜ã„ã—ã¾ã™");
    ASSERT_TRUE(document.is_rejected == false);
    ASSERT_TRUE(document.metadata.find("EmojiRemover") != document.metadata.end());
    ASSERT_TRUE(document.language == "");       
    
    document.text = "ã“ã‚“ã«ã¡ã¯ğŸ˜—ã‚ˆã‚ã—ããŠé¡˜ã„ã—ã¾ã™ğŸ¤¡ğŸ¤¡";
    corpus_cleaner.EmojiRemover(document);
    ASSERT_TRUE(document.text == "ã“ã‚“ã«ã¡ã¯ã‚ˆã‚ã—ããŠé¡˜ã„ã—ã¾ã™");
    
    
    document.text = "ğŸ¤—ãŠğŸ¤—ã¯ğŸ¤—ã‚ˆğŸ¤—ã†ğŸ¤—";
    corpus_cleaner.EmojiRemover(document);
    ASSERT_TRUE(document.text == "ãŠã¯ã‚ˆã†");
    
    document.text = "å¢ƒç•Œå€¤ï¼‘ğŸŒ€";
    corpus_cleaner.EmojiRemover(document);
    ASSERT_TRUE(document.text == "å¢ƒç•Œå€¤ï¼‘");

    document.text = "å¢ƒç•Œå€¤ï¼’ğŸ§¿";
    corpus_cleaner.EmojiRemover(document);
    ASSERT_TRUE(document.text == "å¢ƒç•Œå€¤ï¼’"); 
}

TEST_F(CorpusCleanerTest, SentenceSegmenter) {
    string input_folder_path = "../data/input/sentence_segment/";
    string output_folder_path = "../data/output/sentence_segment/";
    string intermediate_folder_path = "../data/output/sentence_segment/intermediate/";
    string answer_folder_path = "../data/answer/sentence_segment/";

    RemoveFolder(intermediate_folder_path);
    RemoveFolder(output_folder_path);
    GenerateDedupLSH generate_dedup_lsh(5,200,20,10);
    LSHDeduplicator deduplicator(true,"../data/output/blacklist.txt",true,5120);
    CorpusCleaner corpus_cleaner(input_folder_path,
                                 output_folder_path,
                                 min_length,
                                 max_length,
                                 accept_language,
                                 true,
                                 true,
                                 0.3,
                                 15000,
                                 &generate_dedup_lsh,
                                 &deduplicator);

    // MoveFolder(output_folder_path+"/cleaned/", output_folder_path+"/intermediate/"); 
    // cout << "MoveFolder Completed" << endl;

    corpus_cleaner.SentenceSegmenter( output_folder_path+"/intermediate/",output_folder_path+"/cleaned/");
    cout << "SentenceSegmentation Completed" << endl;
    vector<string> file_list;
    GetFileNameListWithoutExtention(answer_folder_path,&file_list);
    for (int i=0;i<(int)file_list.size();i++){
        ASSERT_TRUE(CompareFiles(output_folder_path+"/cleaned/"+file_list[i]+".jsonl",answer_folder_path+"/"+file_list[i]+".jsonl"));
    }
}


TEST_F(CorpusCleanerTest, Normalizer) {
    //original
    ASSERT_TRUE("Hello,C++!" == NormalizeNeologd("   Hello, C++!   "));// TODO: Write the comment that this normalizer is don't applied for English text. Because spaces are removed.
    ASSERT_TRUE("-" == NormalizeNeologd("Ë—ÖŠâ€â€‘â€’â€“âƒâ»â‚‹âˆ’"));
    ASSERT_TRUE("-" == NormalizeNeologd("ï¼"));
    ASSERT_TRUE("ãƒ¼" == NormalizeNeologd("ï¹£â€”â€•â”€â”ãƒ¼ï½°"));
    ASSERT_TRUE("ï¼" == NormalizeNeologd("="));
    ASSERT_TRUE("0123456789" == NormalizeNeologd("ï¼ï¼‘ï¼’ï¼“ï¼”ï¼•ï¼–ï¼—ï¼˜ï¼™"));
    ASSERT_TRUE("ABCDEFGHIJKLMNOPQRSTUVWXYZ" == NormalizeNeologd("ï¼¡ï¼¢ï¼£ï¼¤ï¼¥ï¼¦ï¼§ï¼¨ï¼©ï¼ªï¼«ï¼¬ï¼­ï¼®ï¼¯ï¼°ï¼±ï¼²ï¼³ï¼´ï¼µï¼¶ï¼·ï¼¸ï¼¹ï¼º"));
    ASSERT_TRUE("abcdefghijklmnopqrstuvwxyz" == NormalizeNeologd("ï½ï½‚ï½ƒï½„ï½…ï½†ï½‡ï½ˆï½‰ï½Šï½‹ï½Œï½ï½ï½ï½ï½‘ï½’ï½“ï½”ï½•ï½–ï½—ï½˜ï½™ï½š"));
    ASSERT_TRUE("!\"#$%&\'()*+,-./:;<>?@[Â¥]^_`{|}" == NormalizeNeologd("ï¼â€ï¼ƒï¼„ï¼…ï¼†â€™ï¼ˆï¼‰ï¼Šï¼‹ï¼Œï¼ï¼ï¼ï¼šï¼›ï¼œï¼ï¼Ÿï¼ ï¼»ï¿¥ï¼½ï¼¾ï¼¿ï½€ï½›ï½œï½"));
    ASSERT_TRUE("ï¼ã€‚ã€ãƒ»ã€Œã€" == NormalizeNeologd("ï¼ã€‚ã€ãƒ»ã€Œã€"));
    ASSERT_TRUE("ãƒãƒ³ã‚«ã‚¯" == NormalizeNeologd("ï¾Šï¾ï½¶ï½¸"));
    ASSERT_TRUE("o-o" == NormalizeNeologd("oâ‚‹o"));
    ASSERT_TRUE("majikaãƒ¼" == NormalizeNeologd("majikaâ”"));
    ASSERT_TRUE("ã‚ã„" == NormalizeNeologd("ã‚ã€°ã„"));
    ASSERT_TRUE("ã‚¹ãƒ¼ãƒ‘ãƒ¼" == NormalizeNeologd("ã‚¹ãƒ¼ãƒ‘ãƒ¼ãƒ¼ãƒ¼ãƒ¼"));
    ASSERT_TRUE("!#" == NormalizeNeologd("!#"));
    ASSERT_TRUE("ã‚¼ãƒ³ã‚«ã‚¯ã‚¹ãƒšãƒ¼ã‚¹" == NormalizeNeologd("ã‚¼ãƒ³ã‚«ã‚¯ã€€ã‚¹ãƒšãƒ¼ã‚¹"));
    ASSERT_TRUE("ãŠãŠ" == NormalizeNeologd("ãŠ             ãŠ"));
    ASSERT_TRUE("ãŠãŠ" == NormalizeNeologd("      ãŠãŠ"));
    ASSERT_TRUE("ãŠãŠ" == NormalizeNeologd("ãŠãŠ      "));
    ASSERT_TRUE("æ¤œç´¢ã‚¨ãƒ³ã‚¸ãƒ³è‡ªä½œå…¥é–€ã‚’è²·ã„ã¾ã—ãŸ!!!" ==NormalizeNeologd("æ¤œç´¢ ã‚¨ãƒ³ã‚¸ãƒ³ è‡ªä½œ å…¥é–€ ã‚’ è²·ã„ ã¾ã—ãŸ!!!"));
    ASSERT_TRUE("ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ C" == NormalizeNeologd("ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ  C"));
    ASSERT_TRUE("PRMLå‰¯èª­æœ¬" == NormalizeNeologd("ã€€ã€€ã€€ï¼°ï¼²ï¼­ï¼¬ã€€ã€€å‰¯ã€€èª­ã€€æœ¬ã€€ã€€ã€€"));
    ASSERT_TRUE("Coding the Matrix" == NormalizeNeologd("Coding the Matrix"));
    ASSERT_TRUE("å—ã‚¢ãƒ«ãƒ—ã‚¹ã®å¤©ç„¶æ°´Sparking Lemonãƒ¬ãƒ¢ãƒ³ä¸€çµã‚Š" == NormalizeNeologd("å—ã‚¢ãƒ«ãƒ—ã‚¹ã®ã€€å¤©ç„¶æ°´ã€€ï¼³ï½ï½ï½’ï½‹ï½‰ï½ï½‡ã€€ï¼¬ï½…ï½ï½ï½ã€€ãƒ¬ãƒ¢ãƒ³ä¸€çµã‚Š"));
    ASSERT_TRUE("å—ã‚¢ãƒ«ãƒ—ã‚¹ã®å¤©ç„¶æ°´-Sparking*Lemon+ãƒ¬ãƒ¢ãƒ³ä¸€çµã‚Š" == NormalizeNeologd("å—ã‚¢ãƒ«ãƒ—ã‚¹ã®ã€€å¤©ç„¶æ°´-ã€€ï¼³ï½ï½ï½’ï½‹ï½‰ï½ï½‡*ã€€ï¼¬ï½…ï½ï½ï½+ã€€ãƒ¬ãƒ¢ãƒ³ä¸€çµã‚Š"));
	// cout << "Normalizing Text is completed." << endl;
}

TEST_F(CorpusCleanerTest, NGramTokenize) {
    GenerateDedupLSH generate_dedupe_lsh(3,200,20,10);

    wstring text = L"ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™ã€‚";
    vector<wstring> ret = generate_dedupe_lsh.NGramTokenize(text, 3);
    for(int i=0;i<(int)ret.size()-3+1;i++)  ASSERT_TRUE(ret[i]==text.substr(i,3));
    ASSERT_TRUE((int)ret.size()==8);

    text = L"ãŠã¯ã‚ˆ";
    ret = generate_dedupe_lsh.NGramTokenize(text, 5);
    ASSERT_TRUE(ret[0]==L"ãŠã¯ã‚ˆ");
    ASSERT_TRUE((int)ret.size()==1);

    text = L"ãŠã¯ã‚ˆã†ğŸ¤—ã”ã–ã„ã¾ã™ã€‚Huggingface";
    ret = generate_dedupe_lsh.NGramTokenize(text, 3);
    for(int i=0;i<(int)ret.size()-3+1;i++)  ASSERT_TRUE(ret[i]==text.substr(i,3));
    ASSERT_TRUE((int)ret.size()==20);
}

TEST_F(CorpusCleanerTest, GetMinhash) {
    GenerateDedupLSH generate_dedupe_lsh(3,200,20,10);

    wstring text = L"ãŠã¯ã‚ˆã†ã”ã–ã„ã¾ã™ã€‚";
    vector<wstring> tokens = generate_dedupe_lsh.NGramTokenize(text, 3);
    uint64_t minhash = generate_dedupe_lsh.GetMinhash(&tokens,0);

    // cout << minhash<<endl;
    ASSERT_TRUE(minhash==5643264886837621032);
}


TEST_F(CorpusCleanerTest, LSHDeduplicator1) {
    // refer: https://github.com/HojiChar/HojiChar/blob/v0.9.0/tests/filters/test_lsh_deduplication.py

    GenerateDedupLSH generate_dedup_lsh(5,200,20,10);
    vector<string> d1 = generate_dedup_lsh.CalculateLSH(L"å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ã€‚åå‰ã¯ã¾ã ç„¡ã„ã€‚ã©ã“ã§ç”Ÿã¾ã‚ŒãŸã‹ã¨ã‚“ã¨è¦‹å½“ãŒã¤ã‹ã¬ã€‚");
    vector<string> d2 = generate_dedup_lsh.CalculateLSH(L"å¾è¼©ã¯é³¥ã§ã‚ã‚‹ã€‚åå‰ã¯ã¾ã ç„¡ã„ã€‚ã©ã“ã§ç”Ÿã¾ã‚ŒãŸã‹ã¨ã‚“ã¨è¦‹å½“ãŒã¤ã‹ã¬ã€‚");
    vector<string> d3 = generate_dedup_lsh.CalculateLSH(L"ç¥‡åœ’ç²¾èˆã®é˜ã®å£°ã€è«¸è¡Œç„¡å¸¸ã®éŸ¿ãã‚ã‚Šã€‚");

    // cout<<"d1:"<<endl;
    // cout<<"[" <<endl;
    // for(auto lsh:d1) cout << lsh <<endl;
    // cout<<"]" <<endl;

    // cout<<"d2:"<<endl;
    // cout<<"[" <<endl;
    // for(auto lsh:d2) cout << lsh <<endl;
    // cout<<"]" <<endl;

    // cout<<"d3:"<<endl;
    // cout<<"[" <<endl;
    // for(auto lsh:d3) cout << lsh <<endl;
    // cout<<"]" <<endl;

    LSHDeduplicator deduplicator(true,"",true,5120);
    ASSERT_TRUE(deduplicator.Apply(&d1)==false);
    ASSERT_TRUE(deduplicator.Apply(&d2)==true);
    ASSERT_TRUE(deduplicator.Apply(&d3)==false);

}

TEST_F(CorpusCleanerTest, LSHDeduplicator2) {
    // refer: https://github.com/HojiChar/HojiChar/blob/v0.9.0/tests/filters/test_lsh_deduplication.py
    
    GenerateDedupLSH generate_dedup_lsh(5,200,20,10);
    vector<string> d1 = generate_dedup_lsh.CalculateLSH(L"å¾è¼©ã¯çŒ«ã§ã™ã€‚ğŸ¤—åå‰ã¯ã¾ã ç„¡ã„ã€‚");
    vector<string> d2 = generate_dedup_lsh.CalculateLSH(L"å¾è¼©ã¯çŒ«ã§ã™ã€‚ğŸ¤—åå‰ã¯ã¾ã ç„¡ã„ã§ã™ã€‚");

    // cout<<"d1:"<<endl;
    // cout<<"[" <<endl;
    // for(auto lsh:d1) cout << lsh <<endl;
    // cout<<"]" <<endl;

    // cout<<"d2:"<<endl;
    // cout<<"[" <<endl;
    // for(auto lsh:d2) cout << lsh <<endl;
    // cout<<"]" <<endl;

    LSHDeduplicator deduplicator(true,"",true,5120);
    ASSERT_TRUE(deduplicator.Apply(&d1)==false);
    ASSERT_TRUE(deduplicator.Apply(&d2)==true);

    //cout << deduplicator.Apply(&d1) << endl;
    //false
    //cout << deduplicator.Apply(&d2) << endl;
    //false
}


TEST_F(CorpusCleanerTest, MinhashDeduplication) {
    string input_folder_path = "../data/input/sentence_deduplicate/";
    string output_folder_path = "../data/output/minhash/";
    string intermediate_folder_path = "../data/output/minhash/intermediate/";
    string answer_folder_path = "../data/answer/minhash";

    RemoveFolder(intermediate_folder_path);
    RemoveFolder(output_folder_path);
    mkdir(output_folder_path.c_str(), 0777);

    GenerateDedupLSH generate_dedup_lsh(4,200,20,10);
    LSHDeduplicator deduplicator(true,"../data/output/minhash/blacklist.txt",true,5120);
    CorpusCleaner corpus_cleaner(input_folder_path,
                                 output_folder_path,
                                 min_length,
                                 max_length,
                                 accept_language,
                                 false,
                                 true,
                                 0.3,
                                 15000,
                                 &generate_dedup_lsh,
                                 &deduplicator);

    mkdir(intermediate_folder_path.c_str(), 0777);
    // mkdir(intermediate_folder_path.c_str(), 0777);
    // MoveFolder(output_folder_path+"/cleaned/", output_folder_path+"/intermediate/"); 

    vector<string> filename_list;
    GetFileNameListWithoutExtention(output_folder_path+"/intermediate/",&filename_list);
    // Execute the each CorpusCleaner processing on all files in the intermediate folder.
    for (auto filename: filename_list){
        // load data
        ifstream input_file(output_folder_path+"/intermediate/"+"/"+filename+".txt");
        string  output_file_path(output_folder_path+"/cleaned/"+"/"+filename+".jsonl");
        string line="";
        uint64_t line_count=0;
        while (getline(input_file, line)) {
            Document document;
            ConvertTextToDocument(line,filename,to_string(line_count),document);
            // Loop processing as many times as cleaner_list
            corpus_cleaner.MinhashDeduplication(document);
            // dump data
            WriteDocumentToJsonl(document,output_file_path);
            line_count++;
        }
        input_file.close();   
    }

    vector<string> file_list;
    GetFileNameListWithoutExtention(answer_folder_path,&file_list);
    for (int i=0;i<(int)file_list.size();i++){
        ASSERT_TRUE(CompareFiles(output_folder_path+"/cleaned/"+"/"+file_list[i]+".jsonl",answer_folder_path+"/"+file_list[i]+".jsonl"));
    }
}

TEST_F(CorpusCleanerTest,LanguageFilter) 
{
    GenerateDedupLSH generate_dedup_lsh(5,200,20,10);
    LSHDeduplicator deduplicator(true,"../data/output/blacklist.txt",true,5120);
    CorpusCleaner corpus_cleaner("../data/input/",
                                 "../data/output/",
                                 min_length,
                                 max_length,
                                 accept_language,
                                 true,
                                 true,
                                 0.3,
                                 15000,
                                 &generate_dedup_lsh,
                                 &deduplicator);
    document.text = "å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ã€‚åå‰ã¯ã¾ã ç„¡ã„ã€‚";
    corpus_cleaner.LanguageFilter(document);
    //document.text isn't changed.
    ASSERT_TRUE(document.text == "å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ã€‚åå‰ã¯ã¾ã ç„¡ã„ã€‚"); 
    ASSERT_TRUE(document.is_rejected==false);
    ASSERT_TRUE(document.metadata.find("LanguageFilter")==document.metadata.end());

    document.text = "I am a cat. No name yet.";
    corpus_cleaner.LanguageFilter(document);
    ASSERT_TRUE(document.language=="__label__en");
    ASSERT_TRUE(document.is_rejected==true);
    ASSERT_TRUE(document.metadata.find("LanguageFilter")!=document.metadata.end());
}

TEST_F(CorpusCleanerTest,LanguageFilter2) 
{
    GenerateDedupLSH generate_dedup_lsh(5,200,20,10);
    LSHDeduplicator deduplicator(true,"../data/output/blacklist.txt",true,5120);
    CorpusCleaner corpus_cleaner("../data/input/",
                                 "../data/output/",
                                 min_length,
                                 max_length,
                                 {"__label__en"},
                                 true,
                                 true,
                                 0.3,
                                 15000,
                                 &generate_dedup_lsh,
                                 &deduplicator);
    document.text = "I am a cat. No name yet.";
    corpus_cleaner.LanguageFilter(document);
    ASSERT_TRUE(document.language=="__label__en");
    ASSERT_TRUE(document.is_rejected==false);

    //under.threshold
    document.text = "ããgugu";
    corpus_cleaner.LanguageFilter(document);
    ASSERT_TRUE(document.language=="__label__en");
    ASSERT_TRUE(document.language_score<0.3);
    ASSERT_TRUE(document.is_rejected==true);
}

TEST_F(CorpusCleanerTest,QuotesRemover) 
{
    GenerateDedupLSH generate_dedup_lsh(5,200,20,10);
    LSHDeduplicator deduplicator(true,"../data/output/blacklist.txt",true,5120);
    CorpusCleaner corpus_cleaner("../data/input/",
                                 "../data/output/",
                                 min_length,
                                 max_length,
                                 accept_language,
                                 true,
                                 true,
                                 0.3,
                                 15000,
                                 &generate_dedup_lsh,
                                 &deduplicator);
    document.text = "è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’ã¾ãŸã¯åŠæ•™å¸«ã‚ã‚Šå­¦ç¿’ï¼ˆè‹±èªç‰ˆï¼‰ã«ã‚ˆã£ã¦è¨“ç·´ãŒè¡Œã‚ã‚Œã‚‹[1]ã€‚";
    corpus_cleaner.QuotesRemover(document);
    ASSERT_TRUE(document.text
                =="è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’ã¾ãŸã¯åŠæ•™å¸«ã‚ã‚Šå­¦ç¿’ï¼ˆè‹±èªç‰ˆï¼‰ã«ã‚ˆã£ã¦è¨“ç·´ãŒè¡Œã‚ã‚Œã‚‹ã€‚");

    document.text = "è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’ã¾ãŸã¯åŠæ•™å¸«ã‚ã‚Šå­¦ç¿’ï¼ˆè‹±èªç‰ˆï¼‰ã«ã‚ˆã£ã¦è¨“ç·´ãŒè¡Œã‚ã‚Œã‚‹{2}ã€‚";
    corpus_cleaner.QuotesRemover(document);
    ASSERT_TRUE(document.text
                =="è‡ªå·±æ•™å¸«ã‚ã‚Šå­¦ç¿’ã¾ãŸã¯åŠæ•™å¸«ã‚ã‚Šå­¦ç¿’ï¼ˆè‹±èªç‰ˆï¼‰ã«ã‚ˆã£ã¦è¨“ç·´ãŒè¡Œã‚ã‚Œã‚‹ã€‚");

    document.text = "ã“ã‚Œã¯æ–‡çŒ®[123]{456}ã‚’å‚ç…§ãã ã•ã„ã€‚";
    corpus_cleaner.QuotesRemover(document);
    ASSERT_TRUE(document.text
                =="ã“ã‚Œã¯æ–‡çŒ®ã‚’å‚ç…§ãã ã•ã„ã€‚");

    document.text = "ã“ã‚Œã¯æ–‡çŒ®[a]ã‚’å‚ç…§ãã ã•ã„ã€‚";
    corpus_cleaner.QuotesRemover(document);
    ASSERT_TRUE(document.text
                =="ã“ã‚Œã¯æ–‡çŒ®[a]ã‚’å‚ç…§ãã ã•ã„ã€‚");
}

TEST_F(CorpusCleanerTest,KenLMPerplexity) 
{
    vector<wstring> sentence_list;
	sentence_list.push_back(L"æ±äº¬ã¯ãƒƒæ™´ã‚Œ");
	sentence_list.push_back(L"æ±äº¬ã¯å…ƒæ°—ã§ã™");
	sentence_list.push_back(L"å¾è¼©ã¯çŒ«ã§ã‚ã‚‹.åå‰ã¯ã¾ã ãªã„.");
	sentence_list.push_back(L"æ±äº¬ã¯æ™´ã‚Œ");
	//sentence_list.push_back(L"æ±äº¬ å¤§é˜ª åå¤å±‹ ç§‹ç”° åƒè‘‰");
	//sentence_list.push_back(L"ã‚ã‚ã‚ã‚ã‚ã‚ã‚");
	//sentence_list.push_back(L"assdofiuslkã‚ï½“ï½‹ï½„ï½ˆï½Šï½‹");

    vector<double> perplexity_list;
    KenLMFilter kenlm_filter;
    for (wstring sentence:sentence_list) {
        perplexity_list.push_back(kenlm_filter.Perplexity(sentence));
        cout << ConvertWstringToUTF8(sentence) << "perplexity:"<<perplexity_list[(int)perplexity_list.size()-1]<<endl;
	}
    // cout << perplexity_list[6]<<endl;
    //ref: https://google.github.io/googletest/reference/assertions.html
    EXPECT_NEAR(perplexity_list[0],21879.531940405483,0.1);//æ±äº¬ã¯ãƒƒæ™´ã‚Œ
    EXPECT_NEAR(perplexity_list[1],24128.70574300623,0.1);//æ±äº¬ã¯å…ƒæ°—ã§ã™
    EXPECT_NEAR(perplexity_list[2],4117.138960278464,0.1);//å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ã€‚åå‰ã¯ã¾ã ãªã„ã€‚
    EXPECT_NEAR(perplexity_list[3],17809.198147089162,0.1);//æ±äº¬ã¯æ™´ã‚Œ
    //ASSERT_TRUE(perplexity_list[4]>15000);//æ±äº¬ å¤§é˜ª åå¤å±‹ ç§‹ç”° åƒè‘‰
    //ASSERT_TRUE(perplexity_list[5]>15000);//ã‚ã‚ã‚ã‚ã‚ã‚ã‚
    //ASSERT_TRUE(perplexity_list[6]>15000);//assdofiuslkã‚ï½“ï½‹ï½„ï½ˆï½Šï½‹

    ASSERT_TRUE(perplexity_list[0]>perplexity_list[3]);//æ±äº¬ã¯ãƒƒæ™´ã‚Œ>æ±äº¬ã¯æ™´ã‚Œ
    ASSERT_TRUE(perplexity_list[1]>perplexity_list[3]);//æ±äº¬ã¯å…ƒæ°—ã§ã™>æ±äº¬ã¯æ™´ã‚Œ
}

TEST_F(CorpusCleanerTest,KenLMPerplexityWithSentencePiece) 
{
    vector<wstring> sentence_list;
	sentence_list.push_back(L"æ±äº¬ã¯ãƒƒæ™´ã‚Œ");
	sentence_list.push_back(L"æ±äº¬ã¯å…ƒæ°—ã§ã™");
	sentence_list.push_back(L"å¾è¼©ã¯çŒ«ã§ã‚ã‚‹. åå‰ã¯ã¾ã ãªã„.");
	sentence_list.push_back(L"æ±äº¬ã¯æ™´ã‚Œ");
	//sentence_list.push_back(L"æ±äº¬ å¤§é˜ª åå¤å±‹ ç§‹ç”° åƒè‘‰");
	//sentence_list.push_back(L"ã‚ã‚ã‚ã‚ã‚ã‚ã‚");
	//sentence_list.push_back(L"assdofiuslkã‚ï½“ï½‹ï½„ï½ˆï½Šï½‹");

    vector<double> perplexity_list;
    KenLMFilter kenlm_filter;
    for (wstring sentence:sentence_list) {
        perplexity_list.push_back(kenlm_filter.PerplexityWithSentencePiece(sentence));
        cout << ConvertWstringToUTF8(sentence) << "perplexity:"<<perplexity_list[(int)perplexity_list.size()-1]<<endl;
	}
    // cout << perplexity_list[6]<<endl;
    //ref: https://google.github.io/googletest/reference/assertions.html
    EXPECT_NEAR(perplexity_list[0],10808.575564708846,0.1);//æ±äº¬ã¯ãƒƒæ™´ã‚Œ
    EXPECT_NEAR(perplexity_list[1],14207.90466675276,0.1);//æ±äº¬ã¯å…ƒæ°—ã§ã™
    EXPECT_NEAR(perplexity_list[2],677.481230499526,0.1);//å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ã€‚åå‰ã¯ã¾ã ãªã„ã€‚
    EXPECT_NEAR(perplexity_list[3],3340.487952615284,0.1);//æ±äº¬ã¯æ™´ã‚Œ
    //ASSERT_TRUE(perplexity_list[4]>15000);//æ±äº¬ å¤§é˜ª åå¤å±‹ ç§‹ç”° åƒè‘‰
    //ASSERT_TRUE(perplexity_list[5]>15000);//ã‚ã‚ã‚ã‚ã‚ã‚ã‚
    //ASSERT_TRUE(perplexity_list[6]>15000);//assdofiuslkã‚ï½“ï½‹ï½„ï½ˆï½Šï½‹

    ASSERT_TRUE(perplexity_list[0]>perplexity_list[3]);//æ±äº¬ã¯ãƒƒæ™´ã‚Œ>æ±äº¬ã¯æ™´ã‚Œ
    ASSERT_TRUE(perplexity_list[1]>perplexity_list[3]);//æ±äº¬ã¯å…ƒæ°—ã§ã™>æ±äº¬ã¯æ™´ã‚Œ
}

TEST_F(CorpusCleanerTest,PerplexityFilter) 
{

    GenerateDedupLSH generate_dedup_lsh(5,200,20,10);
    LSHDeduplicator deduplicator(true,"../data/output/blacklist.txt",true,5120);
    CorpusCleaner corpus_cleaner("../data/input/",
                                 "../data/output/",
                                 min_length,
                                 max_length,
                                 accept_language,
                                 true,
                                 true,
                                 0.3,
                                 15000,
                                 &generate_dedup_lsh,
                                 &deduplicator);
    Document document;
    document.text = "æ±äº¬ã¯ãƒƒæ™´ã‚Œ";
    corpus_cleaner.PerplexityFilter(document);
    ASSERT_TRUE(document.perplexity<=15000);
    ASSERT_TRUE(document.is_rejected==false);
    ASSERT_TRUE(document.metadata.find("PerplexityFilter")==document.metadata.end());

    document.text = "æ±äº¬ã¯å…ƒæ°—ã§ã™";
    corpus_cleaner.PerplexityFilter(document);
    ASSERT_TRUE(document.perplexity<=15000);
    ASSERT_TRUE(document.is_rejected==false);

    document.text = "å¾è¼©ã¯çŒ«ã§ã‚ã‚‹ã€‚åå‰ã¯ã¾ã ãªã„ã€‚";
    corpus_cleaner.PerplexityFilter(document);
    ASSERT_TRUE(document.perplexity<=15000);
    ASSERT_TRUE(document.is_rejected==false);

    document.text = "æ±äº¬ã¯æ™´ã‚Œ";
    corpus_cleaner.PerplexityFilter(document);
    ASSERT_TRUE(document.perplexity<=15000);
    ASSERT_TRUE(document.is_rejected==false);

    document.text = "æ±äº¬ å¤§é˜ª åå¤å±‹ ç§‹ç”° åƒè‘‰";
    document.is_rejected=false;
    corpus_cleaner.PerplexityFilter(document);
    ASSERT_TRUE(document.perplexity>15000);
    ASSERT_TRUE(document.is_rejected==true);
    ASSERT_TRUE(document.metadata.find("PerplexityFilter")!=document.metadata.end());

    document.text = "ã‚ã‚ã‚ã‚ã‚ã‚ã‚";
    document.is_rejected=false;
    corpus_cleaner.PerplexityFilter(document);
    ASSERT_TRUE(document.perplexity<15000);
    ASSERT_TRUE(document.is_rejected==false);

    document.text = "assdofiuslkã‚ï½“ï½‹ï½„ï½ˆï½Šï½‹";
    document.is_rejected=false;
    corpus_cleaner.PerplexityFilter(document);
    ASSERT_TRUE(document.perplexity<15000);
    ASSERT_TRUE(document.is_rejected==false);


}

TEST_F(CorpusCleanerTest,GetFileNameListWithoutExtention) 
{
    string input_path = "../data/input/sentence_deduplicate";
    vector<string> file_name_list;
    GetFileNameListWithoutExtention(input_path, &file_name_list);
    // cout << file_name_list[0] << endl;
    ASSERT_TRUE(file_name_list[0]=="test_SentenceDeduplication2");
    // cout << file_name_list[1] << endl;
    ASSERT_TRUE(file_name_list[1]=="test_SentenceDeduplication3");
    // cout << file_name_list[2] << endl;
    ASSERT_TRUE(file_name_list[2]=="test_SentenceDeduplication");
}


TEST_F(CorpusCleanerTest,ConvertTextToDocument) 
{
    Document document;
    string sentence = "ã“ã‚“ã«ã¡ã¯ã€‚ç§ã¯celeryã§ã™ã€‚";
    string filename = "input";
    string file_line_count = "0";
    ConvertTextToDocument(sentence,filename,file_line_count,document);
    ASSERT_TRUE(document.text==sentence);
    ASSERT_TRUE(document.id=="input_0");
    ASSERT_TRUE(document.is_rejected==false);
    ASSERT_TRUE(document.language=="");
    ASSERT_TRUE(document.language_score==-1);
    ASSERT_TRUE(document.perplexity==-1);
}

TEST_F(CorpusCleanerTest,WriteDocumentToJsonl) 
{
    Document document;
    string sentence = "ã“ã‚“ã«ã¡ã¯ã€‚ç§ã¯celeryã§ã™ã€‚";
    string filename = "input";
    string file_line_count = "0";
    ConvertTextToDocument(sentence,filename,file_line_count,document);
    if (fs::exists("../data/output/write_document_to_jsonl.jsonl")) {
        fs::remove("../data/output/write_document_to_jsonl.jsonl");
    }

    GenerateDedupLSH generate_dedup_lsh(5,200,20,10);
    LSHDeduplicator deduplicator(true,"../data/output/blacklist.txt",true,5120);
    CorpusCleaner corpus_cleaner("../data/input/",
                                 "../data/output/",
                                 min_length,
                                 max_length,
                                 accept_language,
                                 true,
                                 true,
                                 0.3,
                                 15000,
                                 &generate_dedup_lsh,
                                 &deduplicator);
    WriteDocumentToJsonl(document,"../data/output/write_document_to_jsonl.jsonl");
    
    document.text = "ã„ã„ã‹ã„! ã‚‚ã£ã¨ã‚‚ã€ã‚€ãšã‹ã—ã„äº‹ã€ã¯! ã€è‡ªåˆ†ã‚’ä¹—ã‚Šè¶Šãˆã‚‹äº‹ã€ã•!";
    document.id  = "rohan_0"; 
    document.is_rejected = true;
    // document.metadata;
    document.language="__label__ja";
    document.language_score=0.003;
    document.perplexity=1.692;
    WriteDocumentToJsonl(document,"../data/output/write_document_to_jsonl.jsonl");
    
    ifstream input("../data/output/write_document_to_jsonl.jsonl");
    string line;
    getline(input,line);
    // cout << "{\"text\":\"ã“ã‚“ã«ã¡ã¯ã€‚ç§ã¯celeryã§ã™ã€‚\",\"id\":\"input_0\",\"is_rejected\":\"0\",\"metadata\":\"\",\"language\":\"\",\"language_score\":\"-1\",\"perplexity\":\"-1\"}" << endl;
    // cout << line << endl;
    ASSERT_TRUE(line==string("{\"text\":\"ã“ã‚“ã«ã¡ã¯ã€‚ç§ã¯celeryã§ã™ã€‚\",\"id\":\"input_0\",\"is_rejected\":\"0\",\"metadata\":\"\",\"language\":\"\",\"language_score\":\"-1\",\"perplexity\":\"-1\"}")); 
    getline(input,line);
    ASSERT_TRUE(line=="{\"text\":\"ã„ã„ã‹ã„! ã‚‚ã£ã¨ã‚‚ã€ã‚€ãšã‹ã—ã„äº‹ã€ã¯! ã€è‡ªåˆ†ã‚’ä¹—ã‚Šè¶Šãˆã‚‹äº‹ã€ã•!\",\"id\":\"rohan_0\",\"is_rejected\":\"1\",\"metadata\":\"\",\"language\":\"__label__ja\",\"language_score\":\"0.003\",\"perplexity\":\"1.692\"}" );    
    input.close();
}

TEST_F(CorpusCleanerTest,ReadDocumentFromJsonlOneLine) 
{
    GenerateDedupLSH generate_dedup_lsh(5,200,20,10);
    LSHDeduplicator deduplicator(true,"../data/output/blacklist.txt",true,5120);
    CorpusCleaner corpus_cleaner("../data/input/",
                                 "../data/output/",
                                 min_length,
                                 max_length,
                                 accept_language,
                                 true,
                                 true,
                                 0.3,
                                 15000,
                                 &generate_dedup_lsh,
                                 &deduplicator);
    Document document;
    string jsonl_line = "{\"text\":\"ã€è¦šæ‚Ÿã€ã¨ã¯!! æš—é—‡ã®è’é‡ã«!! é€²ã‚€ã¹ãé“ã‚’åˆ‡ã‚Šé–‹ãäº‹ã ãƒƒ!\",\"id\":\"jorno_0\",\"is_rejected\":\"1\",\"metadata\":\"\",\"language\":\"__label__ja\",\"language_score\":\"0.003\",\"perplexity\":\"1.692\"}";
    
    ReadDocumentFromJsonlOneLine(document,jsonl_line);
    ASSERT_TRUE(document.text=="ã€è¦šæ‚Ÿã€ã¨ã¯!! æš—é—‡ã®è’é‡ã«!! é€²ã‚€ã¹ãé“ã‚’åˆ‡ã‚Šé–‹ãäº‹ã ãƒƒ!");
    ASSERT_TRUE(document.id=="jorno_0");
    ASSERT_TRUE(document.is_rejected==true);
    ASSERT_TRUE(document.language=="__label__ja");
    EXPECT_NEAR(document.language_score,0.003,0.0001);
    EXPECT_NEAR(document.perplexity,1.692,0.0001);    
}

TEST_F(CorpusCleanerTest,ExceptionReadDocumentFromJsonlOneLine)
{
    GenerateDedupLSH generate_dedup_lsh(5,200,20,10);
    LSHDeduplicator deduplicator(true,"../data/output/blacklist.txt",true,5120);
    CorpusCleaner corpus_cleaner("../data/input/",
                                 "../data/output/",
                                 min_length,
                                 max_length,
                                 accept_language,
                                 true,
                                 true,
                                 0.3,
                                 15000,
                                 &generate_dedup_lsh,
                                 &deduplicator);
    Document document;
    // jsonl_line without data
    string jsonl_line = "{}";
    EXPECT_ANY_THROW(ReadDocumentFromJsonlOneLine(document,jsonl_line));
}